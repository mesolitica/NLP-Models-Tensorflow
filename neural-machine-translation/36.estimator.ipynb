{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, n_words):\n",
    "    count = [['GO', 0], ['PAD', 1], ['EOS', 2], ['UNK', 3]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 0)\n",
    "        if index == 0:\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len from: 100, len to: 100\n"
     ]
    }
   ],
   "source": [
    "with open('english-train', 'r') as fopen:\n",
    "    text_from = fopen.read().lower().split('\\n')[:100]\n",
    "with open('vietnam-train', 'r') as fopen:\n",
    "    text_to = fopen.read().lower().split('\\n')[:100]\n",
    "print('len from: %d, len to: %d'%(len(text_from), len(text_to)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab from size: 553\n",
      "Most common words [('.', 109), ('the', 90), (',', 86), ('to', 68), ('of', 59), ('we', 41)]\n",
      "Sample data [142, 152, 115, 5, 194, 159, 14, 62, 428, 12] ['rachel', 'pike', ':', 'the', 'science', 'behind', 'a', 'climate', 'headline', 'in']\n"
     ]
    }
   ],
   "source": [
    "concat_from = ' '.join(text_from).split()\n",
    "vocabulary_size_from = len(list(set(concat_from)))\n",
    "data_from, count_from, dictionary_from, rev_dictionary_from = build_dataset(concat_from, vocabulary_size_from)\n",
    "print('vocab from size: %d'%(vocabulary_size_from))\n",
    "print('Most common words', count_from[4:10])\n",
    "print('Sample data', data_from[:10], [rev_dictionary_from[i] for i in data_from[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab to size: 581\n",
      "Most common words [('.', 105), (',', 73), ('có', 37), ('tôi', 35), ('chúng', 35), ('bạn', 33)]\n",
      "Sample data [45, 23, 260, 196, 11, 190, 53, 32, 26, 88] ['khoa', 'học', 'đằng', 'sau', 'một', 'tiêu', 'đề', 'về', 'khí', 'hậu']\n"
     ]
    }
   ],
   "source": [
    "concat_to = ' '.join(text_to).split()\n",
    "vocabulary_size_to = len(list(set(concat_to)))\n",
    "data_to, count_to, dictionary_to, rev_dictionary_to = build_dataset(concat_to, vocabulary_size_to)\n",
    "print('vocab to size: %d'%(vocabulary_size_to))\n",
    "print('Most common words', count_to[4:10])\n",
    "print('Sample data', data_to[:10], [rev_dictionary_to[i] for i in data_to[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GO = dictionary_from['GO']\n",
    "PAD = dictionary_from['PAD']\n",
    "EOS = dictionary_from['EOS']\n",
    "UNK = dictionary_from['UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatbot:\n",
    "    def __init__(self, size_layer, num_layers, embedded_size, batch_size,\n",
    "                 from_dict_size, to_dict_size, grad_clip=5.0):\n",
    "        self.size_layer = size_layer\n",
    "        self.num_layers = num_layers\n",
    "        self.embedded_size = embedded_size\n",
    "        self.grad_clip = grad_clip\n",
    "        self.from_dict_size = from_dict_size\n",
    "        self.to_dict_size = to_dict_size\n",
    "        self.batch_size = batch_size\n",
    "        self.model = tf.estimator.Estimator(self.model_fn)\n",
    "        \n",
    "    def lstm_cell(self, reuse=False):\n",
    "        return tf.nn.rnn_cell.LSTMCell(self.size_layer, reuse=reuse)\n",
    "    \n",
    "    def seq2seq(self, x_dict, reuse):\n",
    "        x = x_dict['x']\n",
    "        x_seq_len = x_dict['x_len']\n",
    "        with tf.variable_scope('encoder', reuse=reuse):\n",
    "            encoder_embedding = tf.get_variable('encoder_embedding') if reuse else tf.get_variable('encoder_embedding', \n",
    "                                                                                                   [self.from_dict_size, self.embedded_size], \n",
    "                                                                                                   tf.float32, tf.random_uniform_initializer(-1.0, 1.0))\n",
    "            _, encoder_state = tf.nn.dynamic_rnn(\n",
    "                cell = tf.nn.rnn_cell.MultiRNNCell([self.lstm_cell() for _ in range(self.num_layers)]), \n",
    "                inputs = tf.nn.embedding_lookup(encoder_embedding, x),\n",
    "                sequence_length = x_seq_len,\n",
    "                dtype = tf.float32)\n",
    "            encoder_state = tuple(encoder_state[-1] for _ in range(self.num_layers))\n",
    "        if not reuse:\n",
    "            y = x_dict['y']\n",
    "            y_seq_len = x_dict['y_len']\n",
    "            with tf.variable_scope('decoder', reuse=reuse):\n",
    "                decoder_embedding = tf.get_variable(\n",
    "                    'decoder_embedding', [self.to_dict_size, self.embedded_size], tf.float32,\n",
    "                    tf.random_uniform_initializer(-1.0, 1.0))\n",
    "                helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "                    inputs = tf.nn.embedding_lookup(decoder_embedding, y),\n",
    "                    sequence_length = y_seq_len,\n",
    "                    time_major = False)\n",
    "                decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                    cell = tf.nn.rnn_cell.MultiRNNCell([self.lstm_cell() for _ in range(self.num_layers)]),\n",
    "                    helper = helper,\n",
    "                    initial_state = encoder_state,\n",
    "                    output_layer = tf.layers.Dense(self.to_dict_size))\n",
    "                decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                    decoder = decoder,\n",
    "                    impute_finished = True,\n",
    "                    maximum_iterations = tf.reduce_max(y_seq_len))\n",
    "                return decoder_output.rnn_output\n",
    "        else:\n",
    "            with tf.variable_scope('decoder', reuse=reuse):\n",
    "                helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "                    embedding = tf.get_variable('decoder_embedding'),\n",
    "                    start_tokens = tf.tile(tf.constant([GO], dtype=tf.int32), [tf.shape(x)[0]]),\n",
    "                    end_token = EOS)\n",
    "                decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                    cell = tf.nn.rnn_cell.MultiRNNCell(\n",
    "                        [self.lstm_cell(reuse=True) for _ in range(self.num_layers)]),\n",
    "                    helper = helper,\n",
    "                    initial_state = encoder_state,\n",
    "                    output_layer = tf.layers.Dense(self.to_dict_size, _reuse=reuse))\n",
    "                decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                    decoder = decoder,\n",
    "                    impute_finished = True,\n",
    "                    maximum_iterations = 2 * tf.reduce_max(x_seq_len))\n",
    "                return decoder_output.sample_id\n",
    "            \n",
    "    def model_fn(self, features, labels, mode):\n",
    "        logits = self.seq2seq(features, reuse=False)\n",
    "        predictions = self.seq2seq(features, reuse=True)\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "        y_seq_len = features['y_len']\n",
    "        masks = tf.sequence_mask(y_seq_len, tf.reduce_max(y_seq_len), dtype=tf.float32)\n",
    "        loss_op = tf.contrib.seq2seq.sequence_loss(logits = logits, targets = labels, weights = masks)\n",
    "        params = tf.trainable_variables()\n",
    "        gradients = tf.gradients(loss_op, params)\n",
    "        clipped_gradients, _ = tf.clip_by_global_norm(gradients, self.grad_clip)\n",
    "        train_op = tf.train.AdamOptimizer().apply_gradients(zip(clipped_gradients, params),\n",
    "                                                            global_step=tf.train.get_global_step())\n",
    "        acc_op = tf.metrics.accuracy(labels=labels, predictions=predictions)\n",
    "        estim_specs = tf.estimator.EstimatorSpec(\n",
    "            mode = mode,\n",
    "            predictions = predictions,\n",
    "            loss = loss_op,\n",
    "            train_op = train_op,\n",
    "            eval_metric_ops = {'accuracy': acc_op})\n",
    "        return estim_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpku_y7zem\n",
      "INFO:tensorflow:Using config: {'_keep_checkpoint_every_n_hours': 10000, '_master': '', '_save_checkpoints_steps': None, '_task_id': 0, '_session_config': None, '_num_worker_replicas': 1, '_model_dir': '/tmp/tmpku_y7zem', '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_save_checkpoints_secs': 600, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fadbf4a1e48>, '_tf_random_seed': None, '_task_type': 'worker', '_service': None, '_is_chief': True, '_log_step_count_steps': 100, '_save_summary_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "size_layer = 256\n",
    "num_layers = 2\n",
    "embedded_size = 256\n",
    "batch_size = len(text_from)\n",
    "model = Chatbot(size_layer, num_layers, embedded_size, batch_size,\n",
    "                vocabulary_size_from + 4, vocabulary_size_to + 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'quality'\n",
      "'mài'\n"
     ]
    }
   ],
   "source": [
    "def str_idx(corpus, dic):\n",
    "    X = []\n",
    "    for i in corpus:\n",
    "        ints = []\n",
    "        for k in i.split():\n",
    "            try:\n",
    "                ints.append(dic[k])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                ints.append(UNK)\n",
    "        X.append(ints)\n",
    "    return X\n",
    "\n",
    "X = str_idx(text_from, dictionary_from)\n",
    "Y = str_idx(text_to, dictionary_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    padded_seqs = []\n",
    "    seq_lens = []\n",
    "    max_sentence_len = max([len(sentence) for sentence in sentence_batch])\n",
    "    for sentence in sentence_batch:\n",
    "        padded_seqs.append(sentence + [pad_int] * (max_sentence_len - len(sentence)))\n",
    "        seq_lens.append(len(sentence))\n",
    "    return np.array(padded_seqs).astype(np.int32), np.array(seq_lens).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x, seq_x = pad_sentence_batch(X, PAD)\n",
    "batch_y, seq_y = pad_sentence_batch(Y, PAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpku_y7zem/model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 6.371539\n",
      "INFO:tensorflow:Saving checkpoints for 100 into /tmp/tmpku_y7zem/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.5187457.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x7fadbf4a14a8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x={'x':batch_x, 'x_len':seq_x, 'y':batch_y, 'y_len':seq_y}, y=batch_y,\n",
    "            batch_size=batch_size, num_epochs=100, shuffle=False)\n",
    "model.model.train(input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
